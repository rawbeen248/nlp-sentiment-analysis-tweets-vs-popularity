{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e56025b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Twitter Post Popularity: A Data-Driven Investigation of the Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8895e4",
   "metadata": {},
   "source": [
    "Main goal of this research is to investigate the corrleation between sentiments of the tweets of popular people with its popularity (number of likes and shares).\n",
    "\n",
    "For this project two different datasets are used. One is labeled (0 and 1 for positive and negative sentiments respectively) dataset and the other one is unlabeled dataset. There is no single dataset with labels and the features required for this investogation therefore this approach is taken.\n",
    "\n",
    "Main features wrequired in a single dataframe to conduct this research are: tweet text, number of likes for that tweet, number of shares for that tweet and the sentiment score for that tweet so that correlation could be computed. As it was not possible to conduct the research using single dataset, the following steps are followed:\n",
    "\n",
    "1. After performing all the required preprocessing of the data, a classifcation model on the labeled dataset is trained.\n",
    "\n",
    "2. Then that trained classification model is used to predict the labels for the unlabeled dataset along with sentiment score which in this case is computed using predict_proba function of the classifier model.\n",
    "\n",
    "3. Then the popularity score is computed using the features: number of likes and number of shares.\n",
    "\n",
    "4. Finally linear and non-linear correlation between the populariy and the sentiment score is computed.\n",
    "\n",
    "The steps are explained along with the code in more details on the notebook.\n",
    "\n",
    "Datasets used: \n",
    "\n",
    "Training Dataset: Dataset from Sentimnet140 (link: https://docs.google.com/file/d/0B04GJPshIjmPRnZManQwWEdTZjg/edit?resourcekey=0-betyQkEmWZgp8z0DFxWsHw)\n",
    "\n",
    "Main Dataset: Available along with the notebook. \n",
    "\n",
    "Libraries: \n",
    "You can find all the required libraries in requirements.txt file. And install those through terminal with this command: \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83536910",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337779b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b2488",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1f97a",
   "metadata": {},
   "source": [
    "This is the first dataset which is labeled (0 for negative sentiment tweets and 1 for positive sentiment tweets). We will use this dataset to train a classification model so that we can use it for predicting labels and sentiment score for our main dataset which is unlabeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10581df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "data = pd.read_csv('trainingdata.csv', encoding='ISO-8859-1', header=None)\n",
    "# naming the columns\n",
    "data = data.rename(columns={0: 'label', 1: 'tweet_id', 2: 'date', 3: 'query', 4: 'user', 5: 'tweet_text'})\n",
    "# replacing 4 with 1 for the positive tweets\n",
    "data['label'] = data['label'].replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c29e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fbc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping query column as we dont need it\n",
    "data = data.drop('query', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09931f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindexing the columns\n",
    "data = data[['tweet_id', 'user', 'date', 'tweet_text', 'label']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0444a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the class ratio in the data\n",
    "\n",
    "num_pos = (data[\"label\"] == 1).sum()\n",
    "num_neg = (data[\"label\"] == 0).sum()\n",
    "\n",
    "print(\"Number of positive tweets:\", num_pos)\n",
    "print(\"Number of negative tweets:\", num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing @mentions from the tweet text\n",
    "# we are doing this by applying a lambda function on the column 'tweet_text'\n",
    "# the lambda function substitutes the '@...' with nothing\n",
    "data_01['tweet_text'] = data_01['tweet_text'].apply(lambda x: re.sub('@[^\\s]+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b37ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing links(urls) from the tweet text\n",
    "# we are doing this by applying a lambda function to the column 'tweet_text'.\n",
    "# the lambda function substitutes the 'https...' with nothing\n",
    "data_01[\"tweet_text\"] = data_01[\"tweet_text\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing #hashtags from the tweet text\n",
    "# we are doing this by applying a lambda function to the column 'tweet_text'.\n",
    "# the lambda function substitutes the '#...' with nothing\n",
    "data_01[\"tweet_text\"] = data_01[\"tweet_text\"].apply(lambda x: re.sub(r\"#\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca747942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the data_01\n",
    "data_02 = data_01.copy()\n",
    "data_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the tweet text \n",
    "# here we are using a lambda function to lowercase the column 'tweet_text'\n",
    "# and storing it in new column called 'tweet_text_cleaned' .\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove all the emojis in the text\n",
    "# we are defining a function that removes (substitutes with nothing) all the emojis that are stored in the list\n",
    "# inside the function when it is applied to a text.\n",
    "# afterwards, we are applying this function to the column 'tweet_text_cleaned'\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966df205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the punctuations from the text\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each string in the text using word_tokenize() function from NLTK library\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words from the text using stopwords module from NLTK library\n",
    "# we are first storing in a set all the stopwrods from language 'english'\n",
    "# afterwards, we are applying a lambda function to remove all the stopwords stored in the set\n",
    "# from the column 'tweet_text_cleaned'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].apply(lambda x: [token for token in x if token not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the words in the text using WordNetLeammatizer from NLTK library\n",
    "# here we are also using a lambda function to apply WordNetLemmatizer to the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the tokens in each tweet into a single string\n",
    "# we are using lambda function for this operation as well\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the dataframe 'data_02'\n",
    "data_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f549a77",
   "metadata": {},
   "source": [
    "### Visualization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85419d",
   "metadata": {},
   "source": [
    "#### Horizontal Bar Plot for 20 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the following code we are plotting 20 most frequent positive words from the column 'tweet_text_cleaned'\n",
    "\n",
    "# For Positive(1) Label\n",
    "\n",
    "sns.set(style = 'white')\n",
    "# Subset positive review dataset\n",
    "all_words_df = data_02[data_02['label'] == 1]\n",
    "\n",
    "# Extracts words into list and count frequency\n",
    "all_words = ' '.join([text for text in all_words_df ['tweet_text_cleaned']])\n",
    "all_words = all_words.split()\n",
    "words_df = FreqDist(all_words)\n",
    "\n",
    "# Extracting words and frequency from words_df object\n",
    "words_df = pd.DataFrame({'word':list(words_df.keys()), 'count':list(words_df.values())})\n",
    "\n",
    "# Subsets top 30 words by frequency\n",
    "words_df = words_df.nlargest(columns=\"count\", n = 20) \n",
    "\n",
    "words_df.sort_values('count', inplace = True)\n",
    "\n",
    "# Plotting 30 frequent words\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = plt.barh(words_df['word'], width = words_df['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the following code we are plotting 20 most frequent negative words from the column 'tweet_text_cleaned'\n",
    "\n",
    "# For Negative(0) Label\n",
    "\n",
    "sns.set(style = 'white')\n",
    "# Subset positive review dataset\n",
    "all_words_df = data_02[data_02['label'] == 0]\n",
    "\n",
    "# Extracts words into list and count frequency\n",
    "all_words = ' '.join([text for text in all_words_df ['tweet_text_cleaned']])\n",
    "all_words = all_words.split()\n",
    "words_df = FreqDist(all_words)\n",
    "\n",
    "# Extracting words and frequency from words_df object\n",
    "words_df = pd.DataFrame({'word':list(words_df.keys()), 'count':list(words_df.values())})\n",
    "\n",
    "# Subsets top 30 words by frequency\n",
    "words_df = words_df.nlargest(columns=\"count\", n = 20) \n",
    "\n",
    "words_df.sort_values('count', inplace = True)\n",
    "\n",
    "# Plotting 30 frequent words\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = plt.barh(words_df['word'], width = words_df['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'im' from all the texts as it had the most frequency for both labels\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].str.replace('im', '')\n",
    "data_02['tweet_text_cleaned'] = data_02['tweet_text_cleaned'].str.strip()\n",
    "data_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32747b",
   "metadata": {},
   "source": [
    "#### World Cloud Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808eadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Word Cloud for positive (1) label\n",
    "# getting the positive values\n",
    "\n",
    "word_cloud_df_pos = data_02[data_02['label'] == 1]\n",
    "\n",
    "# joining the positive words and storing them in the variable all_words_pos\n",
    "all_words_pos = ' '.join([text for text in word_cloud_df_pos['tweet_text_cleaned']])\n",
    "# building a word cloud with the positive words\n",
    "wordcloud_pos = WordCloud(width = 800, height = 800, \n",
    "                      background_color ='white', \n",
    "                      min_font_size = 10).generate(all_words_pos)\n",
    "\n",
    "#plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_pos) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilding a Word Cloud for negative (0) label\n",
    "# getting the negative values\n",
    "\n",
    "word_cloud_df_neg = data_02[data_02['label'] == 0]\n",
    "\n",
    "# joining the negative words and storing them in the variable all_words_neg\n",
    "all_words_neg = ' '.join([text for text in word_cloud_df_neg['tweet_text_cleaned']])\n",
    "# building a word cloud with the negative words\n",
    "wordcloud_neg = WordCloud(width = 800, height = 800, \n",
    "                      background_color ='white', \n",
    "                      min_font_size = 10).generate(all_words_neg)\n",
    "\n",
    "#plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_neg) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the copy of data_02 in data_final \n",
    "data_final = data_02.copy()\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc24c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just choosing the feature that will be used to train the model and dropping the rest of the columns\n",
    "data_final = data_final.drop(['tweet_id', 'user', 'date', 'tweet_text'], axis=1)\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a regular expression tokenizer \n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "# creating a count vectorizer object with English stop words and unigram tokens\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "# fitting the count vectorizer to the tweet text and transforming the text into matrix of token counts\n",
    "text_counts = cv.fit_transform(data_final['tweet_text_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfda29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into trainig and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data_final['label'], test_size=0.25, random_state=5, stratify=data_final['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tunning with GridSearchCV for Multinomial Naive Bayes classifier\n",
    "# this is done by at first storing possible alpha values in 'parameters', afterwards applying GridSearchCV for\n",
    "# MNB to the parameters and fitting it to X_train and y_train.\n",
    "parameters = {'alpha': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]}\n",
    "MNB = MultinomialNB()\n",
    "clf = GridSearchCV(MNB, parameters)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Printing the best parameters and the best score\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model with the best parameter\n",
    "MNB = MultinomialNB(alpha=3.5)\n",
    "MNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on the testing dataset and printing the accuracy score\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(\"Accuracy Score: \",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e20727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying confusion matrix and computing precision, recall, and F1 score\n",
    "cm = confusion_matrix(Y_test, predicted)\n",
    "\n",
    "precision = precision_score(Y_test, predicted)\n",
    "recall = recall_score(Y_test, predicted)\n",
    "f1 = f1_score(Y_test, predicted)\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1 score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f3ddd",
   "metadata": {},
   "source": [
    "## Main Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6ad35",
   "metadata": {},
   "source": [
    "This is our main dataset which doesnot have labels but does have number of likes and number of shares features. Now as we already have trained a classification model. We will be using it to predict the label and the sentiment score for each tweet in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "df = pd.read_csv('clean_tweets_final.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing null values\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b30ded",
   "metadata": {},
   "source": [
    "##### Applying the same approaches for preprocessing as for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b85c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove punctuation marks\n",
    "# here we define 'pattern' that stores all the punctuations\n",
    "# afterwards, we define 'text_without_punct' that stores all the text without punctuations. this is done by \n",
    "# substituting the punctuations stored in side 'pattern' with nothing (''). Afterwards, inside the same column\n",
    "# we replace _ with space (' ') and return the 'text_without_punct' \n",
    "# finally, we apply this function to the 'content_without_stopwords' column\n",
    "def remove_punctuation(text):\n",
    "    pattern = r'[^\\w\\s_]'\n",
    "    text_without_punct = re.sub(pattern, '', text)\n",
    "    # Replace underscores with spaces\n",
    "    text_without_punct = text_without_punct.replace('_', ' ')\n",
    "    return text_without_punct\n",
    "\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00721f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'im' from all the texts as it had the most frequency for both labels in this dataset as well\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].str.replace('im', '')\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].str.strip()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each string in the text using word_tokenize() function from NLTK library\n",
    "# this is done using a lambda function\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7def38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words from the text using stopwords module from NLTK library\n",
    "# we are storing the stopwords from english language inside 'stop_words'\n",
    "# afterwards, we are using a lambda function to remove these stopwords from the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].apply(lambda x: [token for token in x if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the words in the text using WordNetLeammatizer from NLTK library\n",
    "# this is done by applying WordNetLemmnatizer() on the column 'content_without_stopwords' with a lambda function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d91dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the tokens in each tweet into a single string\n",
    "# this is also done using a lambda function\n",
    "df['content_without_stopwords'] = df['content_without_stopwords'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adba16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the copy of df as df_pretrained\n",
    "df_pretrained = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the feature column from the dataframe\n",
    "feature_df = df['content_without_stopwords']\n",
    "# preprocessing the feature data using the same CountVectorizer that we used during training\n",
    "feature_counts = cv.transform(feature_df)\n",
    "# getting the predicted probabilities for each class\n",
    "label_probabilities = MNB.predict_proba(feature_counts)\n",
    "sentiment_scores = label_probabilities[:, 1]\n",
    "# adding sentiment_score column in the dataframe (1 being extremely positive and 0 being extremely negative)\n",
    "df['sentiment_score'] = sentiment_scores\n",
    "# adding a label column to the dataframe (0 or 1)\n",
    "predicted_labels = MNB.predict(feature_counts)\n",
    "df['label'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e87e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the copy of df inside df_final\n",
    "df_final = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87312e90",
   "metadata": {},
   "source": [
    "### Visualization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the most common positive 20 words\n",
    "\n",
    "# For Positive(1) Label\n",
    "\n",
    "sns.set(style = 'white')\n",
    "# Subset positive review dataset\n",
    "all_words_df = df_final[df_final['label'] == 1]\n",
    "\n",
    "# Extracts words into list and count frequency\n",
    "all_words = ' '.join([text for text in all_words_df ['content_without_stopwords']])\n",
    "all_words = all_words.split()\n",
    "words_df = FreqDist(all_words)\n",
    "\n",
    "# Extracting words and frequency from words_df object\n",
    "words_df = pd.DataFrame({'word':list(words_df.keys()), 'count':list(words_df.values())})\n",
    "\n",
    "# Subsets top 30 words by frequency\n",
    "words_df = words_df.nlargest(columns=\"count\", n = 20) \n",
    "\n",
    "words_df.sort_values('count', inplace = True)\n",
    "\n",
    "# Plotting 30 frequent words\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = plt.barh(words_df['word'], width = words_df['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the most common 20 negative words\n",
    "\n",
    "# For Negative(0) Label\n",
    "\n",
    "sns.set(style = 'white')\n",
    "# Subset positive review dataset\n",
    "all_words_df = df_final[df_final['label'] == 0]\n",
    "\n",
    "# Extracts words into list and count frequency\n",
    "all_words = ' '.join([text for text in all_words_df ['content_without_stopwords']])\n",
    "all_words = all_words.split()\n",
    "words_df = FreqDist(all_words)\n",
    "\n",
    "# Extracting words and frequency from words_df object\n",
    "words_df = pd.DataFrame({'word':list(words_df.keys()), 'count':list(words_df.values())})\n",
    "\n",
    "# Subsets top 30 words by frequency\n",
    "words_df = words_df.nlargest(columns=\"count\", n = 20) \n",
    "\n",
    "words_df.sort_values('count', inplace = True)\n",
    "\n",
    "# Plotting 30 frequent words\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = plt.barh(words_df['word'], width = words_df['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa6856",
   "metadata": {},
   "source": [
    "#### World Cloud Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Word Cloud for positive (1) label\n",
    "# identifying positive words\n",
    "\n",
    "word_cloud_df_pos = df_final[df_final['label'] == 1]\n",
    "# joining the identified positive words\n",
    "all_words_pos = ' '.join([text for text in word_cloud_df_pos['content_without_stopwords']])\n",
    "# building a word cloud with the positive words\n",
    "wordcloud_pos = WordCloud(width = 800, height = 800, \n",
    "                      background_color ='white', \n",
    "                      min_font_size = 10).generate(all_words_pos)\n",
    "\n",
    "#plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_pos) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7688ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilding a Word Cloud for negative (0) label\n",
    "# identifying the negative words\n",
    "\n",
    "word_cloud_df_neg = df_final[df_final['label'] == 0]\n",
    "# joining the negative words\n",
    "all_words_neg = ' '.join([text for text in word_cloud_df_neg['content_without_stopwords']])\n",
    "# creating a word cloud with the negative words\n",
    "wordcloud_neg = WordCloud(width = 800, height = 800, \n",
    "                      background_color ='white', \n",
    "                      min_font_size = 10).generate(all_words_neg)\n",
    "\n",
    "#plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_neg) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the copy of df_final in df_conclusion\n",
    "df_conclusion = df_final.copy()\n",
    "df_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0071f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column in df_conclusion called 'popularity' based on columns number_of_likes and\n",
    "#number_of_shares\n",
    "df_conclusion['popularity'] = df_conclusion['number_of_likes'] + 3 * df_conclusion['number_of_shares']\n",
    "df_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of sentiment_score vs popularity\n",
    "plt.scatter(df_conclusion['sentiment_score'], df_conclusion['popularity'])\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Popularity')\n",
    "plt.title('Sentiment Score vs Popularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21827b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Pearson correlation coefficients to see linear relationship\n",
    "pearson_correlation = df_conclusion['sentiment_score'].corr(df_conclusion['popularity'])\n",
    "\n",
    "print(f\"Pearson Rank Correlation Coefficient between sentiment score and popularity:\", pearson_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Spearmans correlation coefficients to see non-linear relationship\n",
    "spearmans_correlation, p_value = spearmanr(df_conclusion['sentiment_score'], df_conclusion['popularity'])\n",
    "\n",
    "print(f\"Spearman's Rank Correlation Coefficient between sentiment score and popularity:\", spearmans_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed07fc",
   "metadata": {},
   "source": [
    "Looking at the values received for both Pearson and Spearman's correlation coefficient, we can conclude that there is neither linear nor non-linear correlation between popularity and sentiment score of tweets in the analyzed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91b19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
